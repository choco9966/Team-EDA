# Support Vector Machine
### Author: 김현우,박주연,이주영,홍정아,이지예,주진영

---
### Intro. to Support Vector Machines (SVM)

SVM의 접근은 서로 다른 클래스를 분류 할 *최적의 분리 경계* 를 찾는 데 초점을 둡니다.

아래 그림 1과 같이 두 클래스(빨간색, 파란색)를 분리하는 경계는 초록색과 같은 선이 있습니다.
![](https://choco9966.github.io/Team-EDA/7week/image/1.png) 

*출처 : Kailash Awati and Eight to Late https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/*

하지만, 위와 같은 경계선 말고도 아래와 같이 (여기서는 3개만 그렸지만) 수 없이 많은 경계선을 그릴 수 있습니다.
![](https://choco9966.github.io/Team-EDA/7week/image/2.png) 

*출처 : Kailash Awati and Eight to Late https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/*

그렇다면, 과연 *최적의 분리 경계* 는 어떻게 설정하냐 라는 의문이 생깁니다. 이를 해결하기 위해 *Margin* 과 *Support Vector* 라는 개념이 나오고, 우리는 아래의 그림과 같이

![](https://choco9966.github.io/Team-EDA/7week/image/3.PNG) 

*출처 : Kailash Awati and Eight to Late https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/*

*마진을 최대화 하는 경게선* 을 이루는 *Support Vector*라는 것을 찾는 방법을 이용합니다.

아래와 같이 수식적으로도 표현할 수 있습니다.
![](https://choco9966.github.io/Team-EDA/7week/image/4.PNG) 

![](https://choco9966.github.io/Team-EDA/7week/image/5.PNG) 

*출처 : Kichun Lee, Ph.D. Department of Industrial Engineering anyang University, Seoul, Korea*

수식으로 증명 할 수도 있으나, 이는 쉬운 작업이 아니므로 따로 정리해서 올리도록 하겠습니다.

다시 돌아와서 아래의 그림이 경계선 하나에 의해서 잘 분류 된 형태라면

![](https://choco9966.github.io/Team-EDA/7week/image/1.png) 

*출처 : Kailash Awati and Eight to Late https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/*

그렇지 않은 경우 또한 많습니다. 우리는 그렇지 않은 경우를 크게 2인 
1. 선형으로 분리 가능 한 경우와   
2. 선형으로 분리 불 가능한 경우  
로 나누어 비교하도록 하겠습니다.

### 선형으로 분리 가능 한 경우 - Soft Margin Classifier (Using Support Vector)
![](https://choco9966.github.io/Team-EDA/7week/image/6.png) 
*출처 : Jaeyoon Han 스터디 자료*

위와 같이 오분류된 데이터를 허용하는 경우르 생각 해 보겠습니다. 4개의 사진 모두 오분류된 데이터를 가지고 있고, 그 수는 각자 다릅니다.
이렇게 오분류된 데이터를 가지고 모델을 만들때 사용하는 개념이 **슬랙 변수(Slack Variable)**입니다. 

- e = 0 : 올바르게 분류되었다.
- 0 < e < 1 : 올바르게 분류되었으나, 마진을 넘어서는 범위에 있다.
- 1 < e : 올바르게 분류되지 않았다.

(*참고:아래의 그림으로 e를 이해하기*)

이제 각각의 데이터 포인트에 대해서 슬랙 변수값을 구한 후 모두 더합니다. 이 더한 값이 얼마나 많은 변수들이 오분류되었는
지 말해줍니다.
![](https://choco9966.github.io/Team-EDA/7week/image/8.PNG) 



하지만 이 값이 너무 커버리면 제대로 분류가 되지 않는 모델이 나옵니다. 따라서 이 값에 한계를 주는 최대 비용
(Maximum Cost)를 설정해줍니다. 서포트 벡터 머신에서 가장 중요한 **Tuning Parameter**인 ***C*** 입니다. 이 값에 따라서 모
델이 굉장히 많이 바뀌게 됩니다.

- **C**값이 크면 : 과소적합 *(Underfitting)*
- **C**값이 작으면 : 과적합 *(Overfitting)*

이 모델을 우리는 소프트 마진 분류기(Soft Margin Classifier) 또는 서포트 벡터 분류기(Support Vector Classifier)
라고 부릅니다. 하지만, 아래와 같이 선형분류가 불가능한 문제는 여전히 남아있습니다. 이때 사용되는 개념이 바로 **Kernel** 이라고 합니다.

### 선형으로 분리 불 가능 한 경우 - Using Kernel Trick
![](https://choco9966.github.io/Team-EDA/7week/image/7.png) 
*선형으로 분리 불 가능한 경우*
*출처 : Jaeyoon Han 스터디 자료*

![](https://choco9966.github.io/Team-EDA/7week/image/kernel.png) 

*출처: Çağrı Emre Akın의 SVM자료 http://cagriemreakin.com/veri-bilimi/kernel-svm-9.html *

**Kernel** 이란 위의 그림과 같이 가중치를 달리하여 차원을 변경(공간을 확장)하는 방법입니다. 빨간색 부분의 중앙이 가중치가 높아 위로 솟아오르고, 그 외 부분은 가중치가 낮아서 변화가 없다고 생각하시면 됩니다.  

자주사용하는 kernel로는 아래와 같은 함수들이 있습니다.
![](https://choco9966.github.io/Team-EDA/7week/image/9.PNG) 

*출처 : Jaeyoon Han 스터디 자료*

#### SVM의 장점,단점
```
Pros: Low generalization error, computationally inexpensive, easy to interpret results
Cons: Sensitive to tuning parameters and kernel choice; natively only handles binary
classification
Works with: Numeric values, nominal values
```

---
+++) 파이썬을 통한 알고리즘 분석은 나중에 다시 하도록 하겠습니다.
Properties of SVM

6 Support vector machines 101
6.1 Separating data with the maximum margin 102
6.2 Finding the maximum margin 104
Framing the optimization problem in terms of our classifier 104
Approaching SVMs with our general framework 106
6.3 Efficient optimization with the SMO algorithm 106
Platt’s SMO algorithm 106 ■ Solving small datasets with the
simplified SMO 107
6.4 Speeding up optimization with the full Platt SMO 112
6.5 Using kernels for more complex data 118
Mapping data to higher dimensions with kernels 118 ■ The radial
bias function as a kernel 119 ■ Using a kernel for testing 122
6.6 Example: revisiting handwriting classification 125
6.7 Summary 127