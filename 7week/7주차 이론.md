# Support Vector Machine
### Author: 김현우,박주연,이주영,홍정아,이지예,주진영

---
### Intro. to Support Vector Machines (SVM)

SVM의 접근은 서로 다른 클래스를 분류 할 *최적의 분리 경계* 를 찾는 데 초점을 둡니다.

아래 그림 1과 같이 두 클래스(빨간색, 파란색)를 분리하는 경계는 초록색과 같은 선이 있습니다.
![](https://choco9966.github.io/Team-EDA/7week/image/1.png) 

*출처 : Kailash Awati and Eight to Late https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/*

하지만, 위와 같은 경계선 말고도 아래와 같이 (여기서는 3개만 그렸지만) 수 없이 많은 경계선을 그릴 수 있습니다.
![](https://choco9966.github.io/Team-EDA/7week/image/2.png) 

*출처 : Kailash Awati and Eight to Late https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/*

그렇다면, 과연 *최적의 분리 경계* 는 어떻게 설정하냐 라는 의문이 생깁니다. 이를 해결하기 위해 *Margin* 과 *Support Vector* 라는 개념이 나오고, 우리는 아래의 그림과 같이

![](https://choco9966.github.io/Team-EDA/7week/image/3.PNG) 

*출처 : Kailash Awati and Eight to Late https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/*

*마진을 최대화 하는 경게선* 을 이루는 *Support Vector*라는 것을 찾는 방법을 이용합니다.

아래와 같이 수식적으로도 표현할 수 있습니다.
![](https://choco9966.github.io/Team-EDA/7week/image/4.PNG) 

![](https://choco9966.github.io/Team-EDA/7week/image/5.PNG) 

*출처 : Kichun Lee, Ph.D. Department of Industrial Engineering anyang University, Seoul, Korea*

수식으로 증명 할 수도 있으나, 이는 쉬운 작업이 아니므로 따로 정리해서 올리도록 하겠습니다.

다시 돌아와서 아래의 그림이 경계선 하나에 의해서 잘 분류 된 형태라면

![](https://choco9966.github.io/Team-EDA/7week/image/1.png) 

*출처 : Kailash Awati and Eight to Late https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/*

그렇지 않은 경우 또한 많습니다. 우리는 그렇지 않은 경우를 크게 2인 
1. 선형으로 분리 가능 한 경우와   
2. 선형으로 분리 불 가능한 경우  
로 나누어 비교하도록 하겠습니다.

#### 선형으로 분리 가능 한 경우 - Soft Margin Classifier (Support Vector Classifier)
![](https://choco9966.github.io/Team-EDA/7week/image/6.png) 
*출처 : Jaeyoon Han 스터디 자료*

#### 선형으로 분리 불 가능 한 경우 - Using Kernel Trick
![](https://choco9966.github.io/Team-EDA/7week/image/7.png) 
*출처 : Jaeyoon Han 스터디 자료*





```
Pros: Low generalization error, computationally inexpensive, easy to interpret results
Cons: Sensitive to tuning parameters and kernel choice; natively only handles binary
classification
Works with: Numeric values, nominal values
```



Properties of SVM

6 Support vector machines 101
6.1 Separating data with the maximum margin 102
6.2 Finding the maximum margin 104
Framing the optimization problem in terms of our classifier 104
Approaching SVMs with our general framework 106
6.3 Efficient optimization with the SMO algorithm 106
Platt’s SMO algorithm 106 ■ Solving small datasets with the
simplified SMO 107
6.4 Speeding up optimization with the full Platt SMO 112
6.5 Using kernels for more complex data 118
Mapping data to higher dimensions with kernels 118 ■ The radial
bias function as a kernel 119 ■ Using a kernel for testing 122
6.6 Example: revisiting handwriting classification 125
6.7 Summary 127
