# Time Series Analysis and Forecasting with Prophet

 **Goal:** 

- Explore the data (ECDF, handle missing values etc).
- Analysis per store type and correlational analysis of stores activity.
- Perform extensive Time Series Analysis (seasonal decomposition, trends, autocorrelation).
- Predict next 6 weeks of sales using Prophet (Facebook methodology).

This notebook mainly focuses on the *Time Series Analysis*. An important topic yet not covered. I use then *new methodology Prophet*, recently introduced by *Facebook,* to predict next 6 week of sales. This methodology has a cool feature of modeling for holidays. Finally, right at the end, I also discuss*advantages and drawbacks of forecasting with Seasonal ARIMA and Prophet.*

As it usually goes, we start with the Exploratory Data Analysis of the main metrics revealing present trends and patterns in the data, giving a solid foundation for the further (causal) analysis. 
<br> 
<br>

![rossmann][1]


  [1]: https://kaggle2.blob.core.windows.net/competitions/kaggle/4594/media/rossmann_banner2.png
  
  
=================================================================================
```
import warnings
warnings.filterwarnings("ignore")

# loading packages
# basic + dates 
import numpy as np
import pandas as pd
from pandas import datetime

# data visualization
import matplotlib.pyplot as plt
import seaborn as sns # advanced vizs
%matplotlib inline

# statistics
from statsmodels.distributions.empirical_distribution import ECDF

# time series analysis
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# prophet by Facebook
from fbprophet import Prophet
```
```
# importing train data to learn
train = pd.read_csv("../input/train.csv", 
                    parse_dates = True, low_memory = False, index_col = 'Date')

# additional store data
store = pd.read_csv("../input/store.csv", 
                    low_memory = False)
```
```
# time series as indexes
> train.index
```
```
DatetimeIndex(['2015-07-31', '2015-07-31', '2015-07-31', '2015-07-31',
               '2015-07-31', '2015-07-31', '2015-07-31', '2015-07-31',
               '2015-07-31', '2015-07-31',
               ...
               '2013-01-01', '2013-01-01', '2013-01-01', '2013-01-01',
               '2013-01-01', '2013-01-01', '2013-01-01', '2013-01-01',
               '2013-01-01', '2013-01-01'],
              dtype='datetime64[ns]', name='Date', length=1017209, freq=None)
```
## Exploratory Data Analysis 
In this first section we go through the train and store data, handle missing values and create new features for further analysis.
```
# first glance at the train set: head and tail
> print("In total: ", train.shape)
> train.head(5)
```

```
In total:  (1017209, 8)
```
##### Short description:
- Sales: the turnover for any given day (target variable).
- Customers: the number of customers on a given day.
- Open: an indicator for whether the store was open: 0 = closed, 1 = open.
- Promo: indicates whether a store is running a promo on that day.
- StateHoliday: indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. 
- SchoolHoliday: indicates if the (Store, Date) was affected by the closure of public schools.

We are dealing with **time series data** so it will probably serve us to extract dates for further analysis. We also have two likely correlated vaiables in the dataset, which can be combined into a new feature.
```
# data extraction
train['Year'] = train.index.year
train['Month'] = train.index.month
train['Day'] = train.index.day
train['WeekOfYear'] = train.index.weekofyear

# adding new variable
train['SalePerCustomer'] = train['Sales']/train['Customers']
train['SalePerCustomer'].describe()
```
```
count    844338.000000
mean          9.493641
std           2.197448
min           2.749075
25%           7.895571
50%           9.250000
75%          10.899729
max          64.957854
Name: SalePerCustomer, dtype: float64
```
On average customers spend about 9.50$ per day. Though there are days with Sales equal to zero.

### ECDF: empirical cumulative distribution function
To get the first impression about continious variables in the data we can plot ECDF.

```
sns.set(style = "ticks")# to format into seaborn 
c = '#386B7F' # basic color for plots
plt.figure(figsize = (12, 6))

plt.subplot(311)
cdf = ECDF(train['Sales'])
plt.plot(cdf.x, cdf.y, label = "statmodels", color = c);
plt.xlabel('Sales'); plt.ylabel('ECDF');

# plot second ECDF  
plt.subplot(312)
cdf = ECDF(train['Customers'])
plt.plot(cdf.x, cdf.y, label = "statmodels", color = c);
plt.xlabel('Customers');

# plot second ECDF  
plt.subplot(313)
cdf = ECDF(train['SalePerCustomer'])
plt.plot(cdf.x, cdf.y, label = "statmodels", color = c);
plt.xlabel('Sale per Customer');
```
![ECDF](https://choco9966.github.io/Team-EDA/9week/image/2.png)

1. Sales, Customers가 0인 비율이 20% 정도. (open=0)
2. Sales의 경우 Customers보다 완만한 경사를 가짐.


About 20% of data has zero amount of sales / customers that we need to deal with and almost 80% of time daily amount of sales was less than 1000. So what about zero sales, is it only due to the fact that the store is closed?

```
print("Closed stores and days which didn't have any sales won't be counted into the forecasts.")
train = train[(train["Open"] != 0) & (train['Sales'] != 0)]

print("In total: ", train.shape)
```
```
Closed stores and days which didn't have any sales won't be counted into the forecasts.
In total:  (844338, 13)
```
|	|Store	|StoreType	|Assortment	|CompetitionDistance	|CompetitionOpenSinceMonth	|CompetitionOpenSinceYear	|Promo2	|Promo2SinceWeek	|Promo2SinceYear	|PromoInterval
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|0	|1	|c	|a	|1270.0	|9.0	|2008.0	|0	|NaN	|NaN	|NaN
|1	|2	|a	|a	|570.0	|11.0	|2007.0	|1	|13.0	|2010.0	|Jan,Apr,Jul,Oct
|2	|3	|a	|a	|14130.0	|12.0	|2006.0	|1	|14.0|	2011.0	|Jan,Apr,Jul,Oct
|3	|4	|c	|c	|620.0	|9.0	|2009.0	|0	|NaN	|NaN	|NaN
|4	|5	|a	|a	|29910.0	|4.0	|2015.0	|0	|NaN	|NaN	|NaN

- Store: a unique Id for each store
- StoreType: differentiates between 4 different store models: a, b, c, d
- Assortment: describes an assortment level: a = basic, b = extra, c = extended
- CompetitionDistance: distance in meters to the nearest competitor store
- CompetitionOpenSince[Month/Year]: gives the approximate year and month of the time the nearest competitor was opened
- Promo2: Promo2 is a continuing a promotion for some stores: 0 = store is not participating, 1 = store is participating
- Promo2Since[Year/Week]: describes the year and calendar week when the store started participating in Promo2
- PromoInterval: describes the consecutive intervals Promo2 is started, naming the months the promotion is started. E.g. "Feb,May,Aug,Nov" means each round starts in February, May, August, November of any given year for that store

```
# missing values?
store.isnull().sum()
```
```
Store                          0
StoreType                      0
Assortment                     0
CompetitionDistance            3
CompetitionOpenSinceMonth    354
CompetitionOpenSinceYear     354
Promo2                         0
Promo2SinceWeek              544
Promo2SinceYear              544
PromoInterval                544
dtype: int64
```

```
# missing values in CompetitionDistance
store[pd.isnull(store.CompetitionDistance)]
```

|	|Store	|StoreType	|Assortment	|CompetitionDistance	|CompetitionOpenSinceMonth	|CompetitionOpenSinceYear	|Promo2	|Promo2SinceWeek	|Promo2SinceYear	|PromoInterval
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|290	|291	|d	|a	|NaN	|NaN	|NaN	|0	|NaN	|NaN	|NaN
|621	|622	|a	|c	|NaN	|NaN	|NaN	|0	|NaN	|NaN	|NaN
|878	|879	|d	|a	|NaN	|NaN	|NaN	|1	|5.0	|2013.0	|Feb,May,Aug,Nov

Apperently this information is simply missing from the data. No particular pattern observed. In this case, it makes a complete sense to replace NaN with the median values (which is twice less that the average).라고는 써있지만, 보다 자세한 분석을 위해 store type = d assortment = a 일때 이런식으로 해보는것도 괜찮아 보인다.
```
# fill NaN with a median value (skewed distribuion)
store['CompetitionDistance'].fillna(store['CompetitionDistance'].median(), inplace = True) #inplace = True 참조값을 return함.
```