## LightGBM vs Xgboost vs Castboost

### What is Ensemble?

#### Ensemble, Hybrid Method
앙상블 기법은 동일한 학습 알고리즘을 사용해서 여러 모델을 학습하는 개념입니다. **백지장도 맞들면 낫다** 라는 속담같이, 성능이 좋은 하나의 모델보다는 성능이 조금 떨어지는 여러 모델이 낫다는 의미입니다. *Bagging* 과 *Boosting* 이 이에 해당합니다.

**동일한 학습 알고리즘**을 사용하는 방법을 **앙상블**이라고 한다면, **서로 다른 모델을 결합**하여 새로운 모델을 만들어내는 방법도 있습니다. 대표적으로 Stacking 이 있으며, 최근 Kaggle 에서 많이 소개된 바 있습니다.

##### Bagging

아래는 Bagging의 대표적인 예인 *Random Forest* 입니다. 의사결정나무를 여러개 만들어서 단순 평균을 취해 새로운 의사결정 나무를 만들고, 그것을 랜덤 포레스트라고 부릅니다.

![](https://choco9966.github.io/Team-EDA/summer2018/Home_credit/Ensemble/image/bagging.png)


##### Boosting

![](https://choco9966.github.io/Team-EDA/summer2018/Home_credit/Ensemble/image/boosting.png)

boosting은 bagging과는 조금 다릅니다. bagging이 parallel했다면 (이전 모델이 다음 모델에 영향을 끼치지 않음), boosting은 sequential하게 움직입니다(이전 모델이 다음 모델에 영향을 끼침). bagging과 boosting의 사진을 자세히 보면 bagging은 노란색 동그라미크기가 동일한 반면에, boosting은 크고 작음이 보일 것입니다. 이 차이는 부스팅은 잘 못 분류된 값에 가중치를 줘서 다음 모델이 이 부분을 깊게 학습할 수 있도록 신호를 보낸것입니다. 하지만 이러한 이유때문에 이상치에는 취약한 면을 보입니다.

##### What is difference LightGBM vs Xgboost vs Castboost

이제 부스팅에 대해서 알아봤으니 부스팅의 대표적인 예(Lightgbm, xgboost, catboost, adaboost, GBM 등)중 캐글에서 가장 많이 쓰이는 Lightgbm, xgboost, catboost에 대해서 알아보도록 하겠습니다.

셋의 차이를 알기전에 먼저 GBM에 대해서 부터 알아보자.

![](https://choco9966.github.io/Team-EDA/summer2018/Home_credit/Ensemble/image/years.png)


자세한 내용은 [pdf파일]()을 참조.
*출처:*
https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db