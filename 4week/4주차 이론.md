> ##### Note: 이번 내용은 Jaeyoon Han님의 자료를 저희 스터디원이 발표 한 내용을 중심으로 추가적인 discussion을 정리했습니다. 
> ##### 회귀분석의 개념과 추가적인 내용 및 파이썬 코드에 관한 부분은 아래의 링크를 참고하시기 바랍니다. 개인적으로 이해한 내용으로 작성한 자료니 틀린 부분이나 모르는 부분은 댓글로 남겨주시기 바랍니다.
> ##### 파이썬링크 :  http://blog.naver.com/choco_9966/221254266558

---
## 1. 지도학습/비지도학습/강화학습의 개념
## 2. 선형회귀법(Linear Regression)
## 3. 선형회귀법 실습 with R
---

## 1. 지도학습/비지도학습/강화학습의 개념

|종류   |개념   |
|---|---|
|지도학습 (Supervised Learning)|목적값을 알고있는 data를 가지고 기게학습 알고리즘을 진행.
|비지도학습 (Unsupervised Learning)|목적값을 모르는 data를 가지고 기게학습 알고리즘을 진행.
|강화학습 (Reinforcement Learning)|위와는 다르게 잘하면 agent가 환경에 따라 기계학습 알고리즘을 진행.

## 2. 선형회귀법(Linear Regression)
Y = b0 + b1x1 + b2x2 + ··· + bpxp 와 같이 x라는 독립변수(independent variable)에 대해서 y라는 종속변수(dependent variable)의 관계를 선형으로 구하는 것을 선형회귀법이라 합니다. Y = b0 + b1x1 같이 하나의 독립변수(x가 한개)인 경우를 단순 선형 회귀(Simple Linear Regression)이라 하고 위 빨간색의 식처럼 독립변수(x)가 여러개인경우를 다중 선형 회귀(Multiple Linear Regression)이라 합니다.

간단하게 시각적으로 이해하면 아래 그림과 같습니다. 빨간색의 점(실제 y값)들 사이의 관계를 파란색 선(회귀선/예측값)을 그어서 확인하는게 저희의 목표입니다. 

![](https://choco9966.github.io/Team-EDA/4week/image/1.png)
*출처: Jaeyoon Han님의 스터디 자료.*

![](https://choco9966.github.io/Team-EDA/4week/image/2.png)
*출처: Jaeyoon Han님의 스터디 자료.*

### 2.1 Linear Regression의 장점, 단점, 이슈
 

장점: 결과의 해석이 쉽고, 계산적으로 간편.

단점: 

1.nonlinear data에서 약한 모습을 보임 

2.data가 많을 경우 underfit한 경향을 보임.

3.feature(독립변수)가 많을 경우 실행이 안되거나 결과가 안좋음.

단점의 해결책:

1. Locally weighted linear regression
2. Ridge and lasso regression / forward, backward stepwise

독립변수 : numeric data

종속변수 : continuous data ---> binary한 경우는 logistic regression에서 이용됨.

### 2.2 General approach to regression
 

1. Collect: Any method.
2. Prepare: We'll need numeric values for regression. Nominal values should be mapped to binary values. (더미변수이용)
3. Analyze: It's helpful to visualized 2D plots. Also, we can visualize the regression weights if we apply shrinkage methods.
4. Train: Find the regression weights.
5. Test: We can measure the R2, or correlation of the predicted value and data, to measure the success of our models.
6. Use: With regression, we can forecast a numeric value for a number of inputs.

*출처: Machine_Learning_in_action page.155 Licensed to Brahim Chaibdraa <chaib@iad.ift.ulaval.ca>*
 

### 2.3 수학적 증명 


![](https://choco9966.github.io/Team-EDA/4week/image/3.jpg)


y(i) : i번째 row (i번째 관측치)에 대한 y값. 
x(i)는 i번째 row (i번째 관측치)에 대한 x값. 
w는 다른곳에서는 beta라고도 하며 선형회귀분석의 계수.




우리는 y의 실제값과 x(i).T * w라는 회귀값(예측값)을 뺀 후, 제곱의 합을 최소화하는 방법을 한다. 이는 한마디로 error의 sum을 최소화 하는 w를 찾는것으로 굉장히 일리가 있는 방법이다. 하지만 w를 찾는 여러가지 방법이 있다. 대표적으로 1. gradient descent 2. OLS (최소자승법/최소제곱법) 3. normal equation 4. bayesian estimation 의 4가지 방법이 있고 우리는 2번째 최소제곱법에 의해서 회귀계수를 구할것이다. 그 이유는 최소제곱법은 몇가지 조건만 만족 할 경우 가장 좋은w를 도출해 내는데, 이걸 우리는 가우스 마코프 정리(Gauss-Markov Theorm)이라 한다. 
가우스 마코프 정리https://blog.naver.com/yunjh7024/220881024021에 잘 나와있으니 꼭 한번 읽어보기 바란다.



자!!! 이제 다시 본론으로 넘어와서 우리는 최소제곱법을 이용해서 가장좋은 w를 도출해 낼건데, 최소제곱법이란 말그대로 위에 수식처럼 실제 y와 예측한 y를 뺀 잔차(residual)의 제곱합을 최소로 하는 것을 의미한다. (이미 시작부터 최소제곱합 하겠다고 쓴거였다.) 그리고 고등학교 미분에서 배웠듯이 어떠한 값을 최소로 만드는 점은 w로 미분했을때 0이 되는 w를 찾으면 된다.

y = (y1,y2,…, ym).T    
x = (x1,x2,…,xm ).T    
w = (w0,w1,…,wp) 여기서p는 독립변수의 갯수를 의미 
ε(residual 잔차) = (ε1,ε2,…εn)

x1 = ( 1, x11, x12 …, x1p ).T 

![](https://choco9966.github.io/Team-EDA/4week/image/4.jpg)

![](https://choco9966.github.io/Team-EDA/4week/image/5.jpg)

![](https://choco9966.github.io/Team-EDA/4week/image/6.jpg)




여기서 나온 w가 우리가 사용할 회귀 계수입니다. 하지만, (xTx)의 역행렬이 존재하지 않을 수 있다는 점입니다. 이 문제는 나중에 다시 다루도록 하겠습니다.





이제 우리는 위에서 구한 w를 통해 파란색 선인 회귀선을 그을 수 있고, 새로운 데이터에 대해서 regression 분석을 할 수 있습니다.

 
![](https://choco9966.github.io/Team-EDA/4week/image/7.png)

출처: Machine_Learning_in_action page.158 Licensed to Brahim Chaibdraa <chaib@iad.ift.ulaval.ca>


3. 선형회귀법 실습 with R
(1) 가정
ㄱ. model에 관한 가정.  
● 종속변수와 독립변수 사이에 선형성 이 성립한다.

ㄴ. error에 관한 가정.  
● 독립변수는 확실하게 측정된 값 으로, 확률적으로 변하는 값이 아니다.   
● 오차항의 평균은 0이며, 분산은 시그마제곱 인 정규 분포를 따른다.   
● 서로 다른 오차항은 독립 이다. (iid) 

ㄷ. predictors에 관한 가정.  
● 독립변수들은 서로 독립적이다.(독립변수간 다중공선성 (multicollinearity)이 적어야 한다.)  

*참고:https://m.blog.naver.com/archiku/50115462325 , https://m.blog.naver.com/meunique/221160090068가정에 대한 내용을 좀 더 자세하게 설명해줍니다.*

일단 이러한 가정은 선형회귀법에서 최소제곱법(OLS)를 사용해서 나오는 부분이고, 이러한 부분이 이용되는 분야는 뒤에 다시 설명하겠습니다.

(2) 실습자료 1

```
data(cars) 
head(cars)
#r에 내장된 cars 데이터를 가지고 speed와 dist간의 선형회귀분석을 진행하겠습니다.

> head(cars)
  speed dist
1     4    2
2     4   10
3     7    4
4     7   22
5     8   16
6     9   10

# 목표 dist ≈ β0 + β1 * speed.
model <- lm(dist ~ speed, data = cars) 
# lm은 R에 내장된 함수로 linear regression을 불러들이는 함수이고, cars라는 data에서 y값으로는 dist를 x값으로는 speed를 넣으라는 명령어 입니다.,
> model

Call:
lm(formula = dist ~ speed, data = cars)

Coefficients:
(Intercept)        speed  
    -17.579        3.932 

# 결론 dist ≈−17.579+3.932×speed.
# 이제 추가적으로 더 자세한 내용을 보기 위해서 summary라는 명령어와 plot이라는 명령어를 입력해 보겠습니다.
> summary(model)

Call:
lm(formula = dist ~ speed, data = cars)

# 실제 y와 예측한 y사이의 차이를 의미합니다.
Residuals:
    Min      1Q  Median      3Q     Max 
-29.069  -9.525  -2.272   9.215  43.201 

# 여기서 Estimate를 통해 베타를 확인하고 뒤의 p-value를 통해 베타(계수)가 유의미한지 확인합니다.
# 둘다 0.05미만으로 유의미하고 signif. codes는 중요한 정도에 따라 오른쪽에서부터 왼쪽으로 중요도가 늘어나고 기호가 바뀝니다.
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -17.5791     6.7584  -2.601   0.0123 *  
speed         3.9324     0.4155   9.464 1.49e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 15.38 on 48 degrees of freedom
# R-squared는 설명의 정도를 알려주는 공식인데, 왼쪽의 R-squared는 row가 증가함에따라 값이 증가해서 Adjusted R-squared를 사용합니다
# 0.64는 전체데이터의 64%를 설명한다는 의미.
Multiple R-squared:  0.6511,	Adjusted R-squared:  0.6438 
# 마지막 p-value는 과연 이러한 회귀분석모델 자체가 유의미한지를 확인하는것으로 유의수준5%에서 의미가 있는것을 확인할 수 있습니다.
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
```

```
par(mfrow = c(2, 2))
plot(model)
```

![](https://choco9966.github.io/Team-EDA/4week/image/8.png)  
왼쪽위에부터 Z 순으로 차근차근 설명해 보겠습니다.

1번째 그림
첫 번째 그림은 잔차(residuals)와 모델에서 예측된 Y값(fitted value, Yhat) 사이의 그래프입니다. 우리가 최소제곱법에서 예측할 때 가능하면 잔차의 크기가 작을 뿐 아니라 분포가 한 곳에 몰리지 않고 일정해야 모델이 좋은 것이라고 할 수 있습니다. 따라서 관측값들이 가능하면 골고루 분포해야 하며 그 평균인 붉은 색 실선이 평행해야 하는데, 이 정도면 어느 정도 통과하는 것 같다고 평가할 수 있지만 완벽하지는 않습니다.

위와 같이 말하는 이유는 잔차와 fitted value 사이의 아무런 관계가 없다는것을 보여주기 위함입니다.
한마디로 위의 가정에서 오차항의 평균이 0이고 분산이 시그마제곱인 정규 분포를 따른다는 점을 보여줍니다.
```
# Independent errors assumption
# 회귀모형을 테스팅 해볼 수 있는 다양한 함수가 있는 lmtest 패키지의 dwtest 함수는 independent errors 가정 만족 여부를 확인할 수 있는 Durbin-Watson Test이다. 이를 통해 내부적으로 autocorrelation(자기상관)이 존재하는지 확인할 수 있다.  by [찌니](https://m.blog.naver.com/meunique/221160090068)
```
![](https://choco9966.github.io/Team-EDA/4week/image/9.PNG)  



2번째 그림(normality가정)
두번쨰 는 normal Q-Q 플롯으로 흔히 Q-Q 플롯으로 불리는 것입니다. 이는 정규분포에 따르는 Quantile과 표본의 Quantile의 비교를 보여주는 것으로 가능한 **직선으로 일치해야** 모델이 적합한 것입니다. 
```
#normality assumption
shapiro.test(model$residuals)
```
![](https://choco9966.github.io/Team-EDA/4week/image/10.PNG)  

3번째 그림 linearity와 constant variance 가정
세 번째는 표준화 잔차의 루트를 씌운 값과 회귀식에서 예측값(적합값)의 분포를 보여줍니다.  가능하면 값들이 한 곳에 몰리지 않고 골고루 분포할 뿐 아니라 잔차의 값이 작을수록 회귀 모형이 좋은 것입니다.  ( 특별한 규칙없이 분포되는게 좋음 )
```
#constant variance assumption
#car 패키지의 ncvTest함수 이용
car::ncvTest(model) 
```
![](https://choco9966.github.io/Team-EDA/4week/image/11.PNG)  


4번째 그림
마지막 그림은 지렛값이라고 하는 레버리지 (Leverage) 값을 보여주는 그림입니다. 지렛값은 시소를 타고 있는 사람을 생각해 보면 이해가 쉽습니다. 만약 여러 사람이 탄 시소에서 중앙에서 가장 먼 거리에 누군가 앉게되면 같은 무게라도 시소 기울기에 큰 영향을 주게 됩니다. 지렛값은 이렇게 **하나의 관측치인데 예측 모형에서 많이 벗어나 있어 전체 모형의 회귀계수에 큰 영향을 주는 값**을 의미합니다. 여기에서는 20번 관측치가 가장 크게 벗어난 것으로 보입니다. 

 지렛값의 크기를 구하는 가장 중요한 방법으로 쿡의 거리 (Cook's distance)가 있습니다. 관측값들이 회귀 모형에 적합해서 큰 영향을 주지 않으면 쿡의 거리는 작은 반면 적합하지 않아서 많이 영향을 주면 쿡의 거리가 길어집니다. 여기서는 관측치 49이 가장 먼 거리에 있으며 0.5 이상으로 나옵니다. 쿡의 거리가 1 이상이면 매우 큰 것이고 0.5도 무시할 수 없는 수준입니다. 

*출처 : https://blog.naver.com/jjy0501/221206148507 // 굉장히 참고 많이했습니다. 여기에 추후 친구에게 빌려준 책에 자세히 내용 나와있는데 그것까지 정리해서 올리도록 하겠습니다.*


다중공선성
```
library(car) 
vif <- vif(lm(dist ~ speed, data = cars))

> vif <- vif(lm(dist ~ speed, data = cars))
Error in vif.default(lm(dist ~ speed, data = cars)) : 
  model contains fewer than 2 terms
# 지금 내용은 단순선형회귀분석이여서 (독립변수가 한개여서) 확인을 못하지만, 다중선형회귀분석에서는 이를 통해 다중공선성이 10이하인것을 확인해야 합니다.
```

+)6/1일 추가
{gvlma} 패키지의 gvlma함수.

```
#library(gvlma)
assumption <- gvlma::gvlma(model)
summary(assumption)
```
![](https://choco9966.github.io/Team-EDA/4week/image/12.PNG)  


