encoding: UTF-8

---
title: 5week 
author: 김현우,박주연,이주영,이지예,주진영,홍정아
date: 2018-4-27
output: html_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
rm(list=ls())
Sys.setlocale('LC_ALL','C')
```


###### loading packages
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2) #graph packages
library(dplyr) 
library(gvlma) #assumption check packages
library(car) #check vif
library(corrplot) #correalation
library(gridExtra)
library(randomForest)
library(xgboost)
library(ggrepel)
library(lmtest) #가정만족하는지 확인.
```


###### loading data
```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

###### data structure
```{r}
str(train) #'data.frame':	15129 obs. of  21 variables:
```

```{r}
summary(train)
```

###### missing data 
```{r}
apply(train,2,function(x) sum(is.na(x))) #NA는 없음.
```

```{r}
ggplot(data=train, aes(x=price)) +
        geom_histogram(fill="blue", binwidth = 10000) +
        scale_x_continuous(breaks= seq(0, 7700000, by=100000)) 
```

```{r}
summary(train$price)
```

```{r}
m <- cor(train)
corrplot(m,method="circle") #method에 따라서 그림이 다름. circle 치면 원형태로 나옴.
```

하지만 변수가 많아서 보기가 불편함. 그래서 price와 상관관계가 높은애들만 따로 추출해줄것임.
```{r}
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
numericVarNames <- names(numericVars) #saving names vector for use later on
#cat('There are', length(numericVars), 'numeric variables')

train_numVar <- train[, numericVars]
cor_numVar <- cor(train_numVar, use="pairwise.complete.obs") #correlations of train numeric variables

#sort on decreasing correlations with price
cor_sorted <- as.matrix(sort(cor_numVar[,'price'], decreasing = TRUE))
 #select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]

corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt")
```

```{r}
p1<- ggplot(data=train, aes(x=sqft_living, y=price))+
        geom_point(col='blue') + geom_smooth(method = "lm", se=FALSE, color="black", aes(group=1)) +                     labs(x='sqft_living') 

p2<- ggplot(data=train, aes(x=sqft_living, y=price)) +
        geom_point(col='blue') + geom_smooth() +  labs(x='sqft_living')

grid.arrange(p1,p2,nrow=2) 

```
method = gam과 lm이 굉장히 다른 모습을 보여줌.


```{r}
ggplot(data=train, aes(x=factor(grade), y=price)) +
        geom_boxplot(col='blue') 
```


```{r}
ys <- ggplot(train, aes(x=sale_year, y=price)) +
        geom_bar(stat='summary', fun.y = "median", fill='blue')+
        geom_label(stat = "count", aes(label = ..count.., y = ..count..)) +
        coord_cartesian(ylim = c(0, 770000)) +
        geom_hline(yintercept=450000, linetype="dashed", color = "red") #dashed line is median price

ms <- ggplot(train, aes(x=sale_month, y=price)) +
        geom_bar(stat='summary', fun.y = "median", fill='blue')+
        geom_label(stat = "count", aes(label = ..count.., y = ..count..)) +
        coord_cartesian(ylim = c(0, 770000)) +
        geom_hline(yintercept=450000, linetype="dashed", color = "red") #dashed line is median price

grid.arrange(ys, ms, widths=c(1,2))
```

###### random forest (finding importance variable)
```{r}
set.seed(2018)
quick_RF <- randomForest(x=train[1:15129,2:21], y=train$price, ntree=100,importance=TRUE)
imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]

ggplot(imp_DF[1:20,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + 
  labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + 
  coord_flip() + 
  theme(legend.position="none")
```

```{r}
ggplot(data=train, aes(x=lat, y=price))+
        geom_point(col='blue') + geom_smooth(method = "lm", se=FALSE, color="black", aes(group=1)) +                     labs(x='lat') 
        #geom_text_repel(aes(label = ifelse(train$price>6000000, rownames(train), '')))

```

```{r}
model <- lm(price~.,data = train)
summary(model)
```
```{r}
par(mfrow = c(2, 2))
plot(model)
```
- normal qq가 1829,9911,7670 3개에 굉장히 흔들림. + 애초에 직선모양이 아님.
- scale-location을 보면 값들의 분포가 일정하지 않은걸 알 수 있음.(그리고 양쪽으로 갈 수록 잔차가 커짐)
- Residuals vs leverage를 보면 3값 9911,1829,1188이 예측치와 distance가 많이 멀음.

```{r}
vif(lm(price~.,data = train))
#Error in vif.default(lm(price ~ ., data = train)) : there are aliased coefficients in the model
#이유는 sqft_basement라는 column이 NA값을 가지고 있어서임.
```
###### Model 수정
- 1. 이상치제거
- 2. sqft_basement제거
- 3. 가정에 부합하게 수저

이상치제거
```{r}
train <- train[-c(1188,1829,7670,9911),]
train <- train %>% filter(price<6000000)
```

sqft_basement제거
```{r}
train <- train %>% select(-sqft_basement)
```

가정에 부합하게 수저
```{r}
train$price <- log(train$price + 1) 
```

```{r}
model <- lm(price~.,data = train)
summary(model)
```
R-squared가 0.7013에서 0.7684로 상승한 것을 확인할 수 있음.

```{r}
par(mfrow = c(2, 2))
plot(model)
```
Normal qq가 좋아져지만, 이젠 아래쪽에서 문제가 좀 있는게 보이고, 나머지는 더 안좋아진것 처럼 보이지만 y값이 달려져서 그렇지 위에보다 좋음.

```{r}
vif(lm(price~.,data = train)) #보통은 10이상이면 제거해준다고 함 sqft_living이 그나마 큰 상황. 5정도로 보는 시각도 있음.
```
```{r}
sqrt(vif(lm(price~.,data = train))) > sqrt(10)
```

```{r}
residualPlots(model)
#https://kin.naver.com/qna/detail.nhn?d1id=11&dirId=1113&docId=212884389&qb=Z3ZsbWE=&enc=utf8&section=kin&rank=1&search_sort=0&spq=0&pid=Tx0v6lpySDossvkKYtdssssss6R-200040&sid=iCXpWENrJvIgAyK53tD2zA%3D%3D 이거 보고 좀더 확장시킬 수 있겠다.
```

위 test의 Null은 "Model is additive"라서 이걸 기각하면 문제가 있다는 의미. 마지막 그래프가 잔차인데, 
```{r}
assumption <- gvlma::gvlma(model)
summary(assumption)
```
Global stat와 link function은 linearity 가정이 충족되었는지를 보여주며, 그렇지 않다면(x에대한) data transformation을 하거나 회귀처럼 선형모델이 아닌 비선형 모델을 사용하는 방법이 있다.

Skewness와 Kurtosis는 normality 가정이 충족되었는지를 보여주며, 그렇지 않다면 Y에 대한 data transformation을 해야 할 수 있다. 

Heteroscedasticity는 constant variance 가정이 충족되었는지를 보여준다.

우리는 Heteroscedasticity(이분산성)가정과 Skewness(왜도)가 충족되지 않은것을 통해서 어느 가정이 틀렸는지 확인할 수 있다. but gvlma를 이용하면 간편하기는 하지만, statistical testing 기법이 갖는 한계점처럼 유의수준 0.05에서 [가정 충족 || 가정 충족하지 않음]의 경계를 잘라 버리다 보니 융통성이 부족하다는 점이 있다. 선형회귀는 이런 가정 충족에 대해서 비교적 robust 한 편이다 보니 이 결과만 보고 비선형적 모델로 바로 넘어가는 등의 속단은 위험할 수 있다고 생각한다. 

참고: 찌니 https://m.blog.naver.com/meunique/221160090068 

```{r}
#normality assumption
shapiro.test(model$residuals)
```

```{r}
#constant variance assumption
car::ncvTest(model)
```

```{r}
#independent errors assumption
lmtest::dwtest(model)
```

```{r}
#선형 가정
car::ceresPlots(model)
```

```{r}
pred <- predict(model, test)
pred <- exp(pred)
pred <- ifelse(pred < 0, 0, pred)
```

```{r}
rmsle <- function(pred, act){
  if(sum(pred < 0) > 0)
    stop("예측값에 0보다 작은 값이 존재합니다. 해당 값을 0으로 만들어주세요.")
  if(length(pred) != length(act))
    stop("예측값과 실제값의 벡터 길이가 다릅니다. 예측값을 다시 확인해주세요.")
  
  len <- length(pred)
  pred <- log(pred + 1)
  act <- log(act + 1)
  msle <- mean((pred - act)^2)
  return(sqrt(msle))
}

cat("[1] rmsle:" , rmsle(pred, test$price))
cat("\n[2] Adjusted R-squared:  0.7684")
```

#using 
#1.bathrooms  sqft_living
train_1 - train %% mutate(bathroomssqft_living = bathroomssqft_living)
test_1 - test %% mutate(bathroomssqft_living = bathroomssqft_living)

model - lm(price~.,data = train_1)
summary(model)
plot(model)
pred - predict(model, test_1)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_1$price) # R^2 = 0.736  rmsle = 0.4835
#2.sqft_living  sqft_above

train_2 - train_1 %% mutate(sqft_abovesqft_living = sqft_abovesqft_living)
test_2 - test_1 %% mutate(sqft_abovesqft_living = sqft_abovesqft_living)

model - lm(price~.,data = train_2)
summary(model)
plot(model)
pred - predict(model, test_2)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_2$price) # R^2 = 0.739  rmsle = 0.4738

#3.grade  sqft_living
train_3 - train_2 %% mutate(gradesqft_living = gradesqft_living)
test_3 - test_2 %% mutate(gradesqft_living = gradesqft_living)
model - lm(price~.,data = train_3)
summary(model)
plot(model)
pred - predict(model, test_3)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_3$price) # R^2 = 0.744  rmsle = 0.3824


m3 - cor(train_3)
corrplot(m3,method=circle) #method에 따라서 그림이 다름. circle 치면 원형태로 나옴.

#4.factor화 
train_4_1 - train
test_4_1 - test
train_4_2 - train_3
test_4_2 - test_3

# factor만 제대로 바꿔줘도 error가 0.38까지 줄어듬.
train_4_1[, c(waterfront,view,condition,sale_year,sale_month)] - 
  lapply(train[, c(waterfront,view,condition,sale_year,sale_month)], as.factor)
test_4_1[, c(waterfront,view,condition,sale_year,sale_month)] - 
  lapply(test[, c(waterfront,view,condition,sale_year,sale_month)], as.factor)

model - lm(price~.,data = train_4_1)
summary(model)
plot(model)
pred - predict(model, test_4_1)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_4_1$price) # R^2 = 0.7038 , error = 0.98

# 우리가 만든 모델
train_4_2[, c(waterfront,view,condition,sale_year,sale_month)] - 
  lapply(train[, c(waterfront,view,condition,sale_year,sale_month)], as.factor)
test_4_2[, c(waterfront,view,condition,sale_year,sale_month)] - 
  lapply(test[, c(waterfront,view,condition,sale_year,sale_month)], as.factor)

model - lm(price~.,data = train_4_2)
summary(model)
plot(model)
pred - predict(model, test_4_2)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_4_1$price) # R^2 = 0.746 , error = 0.408

#5.변수들의 변경.
#ㄱ.yr_renovated
yr_renovated_train - ifelse(train$yr_renovated  0.5, 0, 1)
yr_renovated_test - ifelse(test$yr_renovated  0.5, 0, 1)

train_4_2_1 - train_4_2
train_4_2_1$yr_renovated - yr_renovated_train
test_4_2_1 - test_4_2
test_4_2_1$yr_renovated - yr_renovated_test


model - lm(price~.,data = train_4_2_1)
summary(model)
plot(model)
pred - predict(model, test_4_2_1)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_4_2_1$price) # R^2 = 0.746 , error = 0.408


#ㄴ.zipcode
zipcode_train - substr(train$zipcode,1,3)
zipcode_test - substr(test$zipcode,1,3)

train_4_2_2 - train_4_2_1
train_4_2_2$zipcode - zipcode_train
test_4_2_2 - test_4_2_1
test_4_2_2$zipcode - zipcode_test

train_4_2_2$zipcode - as.factor(train_4_2_2$zipcode)
test_4_2_2$zipcode - as.factor(test_4_2_2$zipcode)

model - lm(price~.,data = train_4_2_2)
summary(model)
plot(model)
pred - predict(model, test_4_2_2)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_4_2_2$price) # R^2 = 0.7418 (0.746에서 감소) , error = 0.4064

#ㄷ. sqft_basement제거 후 다중공선성 확인 
train_4_2_2 - train_4_2_2 %% select(-sqft_basement)
test_4_2_2 - test_4_2_2 %% select(-sqft_basement)
model - lm(price~.,data = train_4_2_2)

library(car)

vif-vif(model)
vif



# 10이상인 bathrooms,sqft_living,grade,sqft_above,sale_year 먼저 제거.
train_4_2_3 - train_4_2_2 %% select(-c(bathrooms,sqft_living,grade,sqft_above,sale_year))
test_4_2_3 - test_4_2_2 %% select(-c(bathrooms,sqft_living,grade,sqft_above,sale_year))
model - lm(price~.,data = train_4_2_3)
summary(model)
plot(model)
pred - predict(model, test_4_2_3)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_4_2_3$price) # R^2 = 0.7234 (0.7418에서 감소) , error = 0.38
vif-vif(model)
vif #많이 깔끔해짐.

studentized - rstudent(model)
table(abs(studentized)3)
outliers - which(abs(studentized)3)
refine_train - train_4_2_3[-outliers, ]

model - lm(price~.,data = refine_train)
summary(model)
plot(model)
pred - predict(model, test_4_2_3)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_4_2_3$price) # R^2 = 0.7606 , error = 0.349

# 10이상인 gradesqft_living 제거.
train_4_2_4 - train_4_2_3[,-18]
test_4_2_4 - test_4_2_3[-18]
model - lm(price~.,data = train_4_2_4)
summary(model)
plot(model)
pred - predict(model, test_4_2_4)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_4_2_4$price) # R^2 = 0.7042 (0.7418에서 감소) , error = 0.48
vif-vif(model)
vif #많이 깔끔해짐.

# 5. 이상치 제거
studentized - rstudent(model)
table(abs(studentized)3)
outliers - which(abs(studentized)3)
refine_train - train_4_2_4[-outliers, ]

model - lm(price~.,data = refine_train)
summary(model)
plot(model)
pred - predict(model, test_4_2_4)
pred - ifelse(pred  0, 0, pred)
rmsle(pred, test_4_2_4$price) # R^2 = 0.7286 (0.7042에서 증가) , error = 0.3
```
