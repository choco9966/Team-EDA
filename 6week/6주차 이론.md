# 6주일차: Decision Tree(의사결정나무)
### Author: 김현우,박주연,이주영,이지예,주진영,홍정아

---
# 의사결정나무(Decision Tree)

의사결정나무는 마치 스무고개를 하듯이 **Rules**에 의해서 **Predictors**로 **Target**을 예측하는 모델입니다.

![](https://choco9966.github.io/Team-EDA/6week/image/1.png)
출처: Jaeyoon Han님의 2017 machine learning study 자료

위의 예에서는 **날씨, 바람, 온도, 습도** 4가지에 따라 Golf를 할지, 안 할지를 예측하는것 입니다.

- 뿌리 노드(root node): 의사결정나무에서 맨위에 있는 사각형. 위의 예시에서는 Yes=9, No=5를 의미.

- 가지(branch): 노드와 노드를 잇는 선. 위의 예시에서는 화살표를 의미

- 잎사귀 노드(leaf node) 혹은 말단 노드(terminal node): 가지가 밑으로 더 이상 생성되지 않는 노드. 위의 예시에서는 마지막 줄 FALSE,TRUE,High,Normal을 의미.

- 중간 노드(internal node): 뿌리 노드와 말단 노드를 제외한 모든 노드. 위의 예시에서는 Sunny,Overcast,Rainy

- 깊이(depth): 뿌리 노드부터 말단 노드까지의 노드 수. 위의 예시에서는 3

의사결정나무는 말단 노드에 존재하는 의사결정 결과를 보다 **순수(pure)** 하게 만드는 모델입니다. 다만 그 결과를 만들기까지 어떤 변수를 이용해서 나눌 것이며, 어떤 기준을 두는가가 모델의 중요한 이슈가 되는겁니다. 이러한 기준으로는 어떻게 노드를 나눌 것인지, 여러 형태의 데이터를 다룰 수 있는지, 과적합을 어떻게 방지할 지 등등 여러개가 있고 주로 사용하는 알고리즘은 아래와 같습니다.

출처: Jaeyoon Han님의 2017 machine learning study 자료

- ID3 (Iterative Dichotomizer 3)
- CART (Classification and Regression Tree)
- C4.5
- C5
- CHAID

참고로 이 중에서 C4.5, C5.0 알고리즘을 가장 많이 사용하고

ID3 => C4.5 => C5.0 순으로 보완하면서 개발하여서 순서대로 학습을 해야 한다고 한다.




그리고 ID3,C4.5, C5.0 은 머신러닝 분야 개발이고,

CART,CHAID 알고리즘은 통계학 분야의 알고리즘이라고 한다"


![](https://choco9966.github.io/Team-EDA/6week/image/4.PNG) 

출처: http://blog.naver.com/trashx/60099037740 각 알고리즘 별 내용도 나와있음.

[출처] 기계학습 알고리즘 - 의사 결정 나무 (Tree Models /Decision tree),https://blog.naver.com/lk3436/220233499988|작성자 RDA"






![](https://choco9966.github.io/Team-EDA/6week/image/5.png) 
참고1: https://stackoverflow.com/questions/9979461/different-decision-tree-algorithms-with-comparison-of-complexity-or-performance

참고2: https://www.quora.com/Do-different-decision-tree-algorithms-offer-significant-differences-in-performance



출처: Jaeyoon Han님의 2017 machine learning study 자료


추후 링크(올릴예정)인 machine learning in action ch3에서 ID3에 대해, ch9에서는 CART를 파이썬으로 보다 상세하게 다룰 에정입니다.

CART (Classification and Regression Tree)​

이진분류를 사용하여 discrete target variable(Classification Tree)와 continuous target variable(Regression Tree)를 예측하는것을 의미합니다. (처음 공부할때는 이거를 잘 몰라서 헷갈렸습니다 ㅋㅋㅋ)

1.Measuring Impurity

①Gini Index for rectangle A containing m records
![](https://choco9966.github.io/Team-EDA/6week/image/1.jpg) 




p = proportion of cases in rectangle A that belong to class k

②Entropy Index for rectangle A containing m records
![](https://choco9966.github.io/Team-EDA/6week/image/2.jpg) 



p = proportion of cases in rectangle A that belong to class k




impurity measures

![](https://choco9966.github.io/Team-EDA/6week/image/6.png) 

출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea //참고로 이거 잘못된거 같은데 지금 새벽1시라 기억이 잘 안남. 추후 생각나면 수정함


한글로 번역해 줘야 하는데... 다들 영어 되시지요??? 믿습니다!!!
2.Impurity and Recursive Partitioning
●Apply a ‘Greedy Search’ strategy
●In each step, we find the split which reduces the impurity measure most
![](https://choco9966.github.io/Team-EDA/6week/image/7.png) 

●For binary categorical variables, it is easy to set ‘Left’ and ‘Right’
●What to do when a numerical input variable exist?
●Possible split values (cut-off) value can be given by sorting
출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 

3.Decision Tree Learning

●Building a Decision Tree

1.First test all attributes and select the on that would function as the best root;

2.Break-up the training set into subsets based on the branches of the root node;

3.Test the remaining attributes to see which ones fit best underneath the branches 
of the root node;

4.Continue this process for all other branches until

a.all examples of a subset are of one type

b.there are no examples left (return majority classification of the parent)

c.there are no more attributes left (default value should be majority classification)


●Determining which attribute is best (Entropy & Gain)
●Entropy (E) is the minimum number of bits needed in order to classify an arbitrary example as yes or no
●E(S) = Sci=1 –pi log2 pi ,
●Where S is a set of training examples,
●c is the number of classes, and
●pi is the proportion of the training set that is of class i
●For our entropy equation 0 log2 0 = 0
●The information gain G(S,A) where A is an attribute
●G(S,A) º E(S) - Sv in Values(A)     (|Sv|  /  |S|)  * E(Sv) 
출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 

4.Example

E(S) = E([9+,5-]) = (-9/14 log2 9/14) + (-5/14 log2 5/14) = 0.94
●Next we will need to calculate the information gain G(S,A) for each attribute A where A is taken from the set 
{Outlook, Temperature, Humidity, Wind}.
1) Outlook
●G(S,Outlook) = E(S) – [5/14 * E(Outlook=sunny) + 4/14 * E(Outlook = overcast) + 5/14 * E(Outlook=rain)]
●G(S,Outlook) = E([9+,5-]) – [5/14*E(2+,3-) + 4/14*E([4+,0-]) + 5/14*E([3+,2-])]
●G(S,Outlook) = 0.94 – [5/14*0.971 + 4/14*0.0 + 5/14*0.971]
●G(S,Outlook) = 0.246  <------ 제일 큼!!!
2) Temperature
●G(S,Temperature) = 0.94 – [4/14*E(Temperature=hot) +   6/14*E(Temperature=mild) +   4/14*E(Temperature=cool)]
●G(S,Temperature) = 0.94 – [4/14*E([2+,2-]) +     6/14*E([4+,2-]) + 4/14*E([3+,1-])]
●G(S,Temperature) = 0.94 – [4/14 + 6/14*0.918 + 4/14*0.811] 
●G(S,Temperature) = 0.029

3) Humidity
●G(S,Humidity) = 0.94 – [7/14*E(Humidity=high) + 7/14*E(Humidity=normal)]
●G(S,Humidity = 0.94 – [7/14*E([3+,4-]) + 7/14*E([6+,1-])]
●G(S,Humidity = 0.94 – [7/14*0.985 + 7/14*0.592]
●G(S,Humidity) = 0.1515
4) Wind
●G(S,Wind) = 0.94 – [8/14*0.811 + 6/14*1.00]
●G(S,Wind) = 0.048

![](https://choco9966.github.io/Team-EDA/6week/image/8.png) 

출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 



의사결정 나무의 장점과 단점 

1. 장점

사용하고,이해하기 쉽다. 규칙들을 생성하고 해석하기 매우 쉽다. 변수 선택과 축소가 자동적이다.

통계적인 가정이 필요하지 않다.  결측치를 다루지 않아도 된다.

2. 단점

수평 또는 수직 분할로 잘 포착되지 않은 구조의 데이터가있는 곳에서는 성능이 좋지 않을 수 있습니다.

프로세스가 한 번에 하나의 변수를 처리하기 때문에 변수 간의 상호 작용을 캡처 할 수 없습니다.

출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 



Random Forests ﻿

오버피팅 문제를 극복하기 위해 Bootstrapping(bagging)을 통해 만든 의사결정나무.







![](https://choco9966.github.io/Team-EDA/6week/image/9.png) 
출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 


![](https://choco9966.github.io/Team-EDA/6week/image/10.png) 
출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 

Random Forest의 장점과 단점 

1. 장점

의사결정나무보다 높은 예측력. 일반화가 되어 과적합 가능성이 낮음

2. 단점

블랙박스 모델(모델의 설명력이 떨어짐)







참고: Classification과 Regression Tree 차이점.




![](https://choco9966.github.io/Team-EDA/6week/image/11.PNG) 

출처: https://webcache.googleusercontent.com/search?q=cache:JMex0zf_qCQJ:https://www.casact.org/education/specsem/f2005/handouts/cart.ppt+&cd=22&hl=ko&ct=clnk&gl=kr 



뒤에는 boosting 기법도 나오는데, boosting 의미와 그 중 하나
인 adaboost는 https://blog.naver.com/choco_9966/221248041919에서 다루었습니다. 하...xgboost랑 gradient boost까지 해야 할지는 고민입니다.

● 부록

출처: https://stats.stackexchange.com/questions/22035/partitioning-trees-in-r-party-vs-rpart

Partitioning trees in R: party vs. rpart
[NB: See update 1 below.] I find that the methodology for rpart is far easier to explain than party. The latter, however, is much more sophisticated and likely to give better models. The way I sometimes explain party is to speak of it as basis for producing local linear (or GLM) models. I build up to this by pointing out that the results for rpart are constant across all elements that fall into the leaf node, i.e. the box/region bounded by the splits. Even if there might be improvements via local models, you don't get anything but a constant prediction.

In contrast, party develops the splits to potentially optimize the models for the regions. It is actually using a different criteria than model optimality, but you need to gauge your own capacity for explaining the difference to determine whether you can explain it well. The papers for it are pretty accessible for a researcher, but may be quite challenging for someone not willing to consider simpler methods like random forests, boosting, etc. Mathematically, I think that party is more sophisticated... Nonetheless, CART models are easier to explain, both in terms of methodology and results, and these provide a decent stepping stone for introducing more sophisticated tree-based models.

In short, I would say that you have to do rpart for clarity, and you can use party for accuracy / peformance, but I wouldn't introduce party without introducing rpart.

Update 1. I based my answer on my understanding of party as it was a year or two ago. It has grown up quite a bit, but I would modify my answer to say that I'd still recommend rpart for its brevity and legacy, should "non-fancy" be an important criterion for your client/collaborator. Yet, I would try to migrate to using more functionality from party, after having introduced someone to rpart. It's better to start small, with loss functions, splitting criteria, etc., in a simple context, before introducing a package and methodology that involve far more involved concepts.
