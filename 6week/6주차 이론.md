> Note: 이번 내용은 Jaeyoon Han님의 자료를 저희 스터디원이 진행 한 발표내용을 중심으로 추가적인 discussion을 정리했습니다. 의사결정나무의 개념과 추가적인 내용 및 파이썬 코드에 관한 부분은 아래의 링크를 참고하시기 바랍니다. 개인적으로 이해한 내용으로 작성한 자료니 틀린 부분이나 모르는 부분은 댓글로 남겨주시기 바랍니다!!!

## 1.파이썬 스터디해서 했던 내용 정리해서 아래 수정해줘야하고, 
## 2.예시 부분 다시 정리해주고  
## 3.마지막으로 R실습자료 올려줘야함.  

--------------------------------------------------------------------------------------------------------------------
# 의사결정나무(Decision Tree)
---
의사결정나무는 마치 스무고개를 하듯이 ﻿Rules﻿에 의해서 Predictors로 Target을 예측하는 모델입니다.

![](https://choco9966.github.io/Team-EDA/6week/image/1.png)
출처: Jaeyoon Han님의 2017 machine learning study 자료

그래프로는 아래와 같이 생각할 수 있습니다.
![](https://choco9966.github.io/Team-EDA/6week/image/2.png) 
출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea

![](https://choco9966.github.io/Team-EDA/6week/image/3.png) 
출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea



먼저 의사결정나무에서 맨위에 있는 사각형을 뿌리 노드(root node)라고 하고, 노드와 노드를 잇는 선을 가지(branch)라고 합니다. 그리고 가지가 밑으로 더 이상 생성되지 않는 노드를 잎사귀 노드(leaf node) 혹은 말단 노드(terminal node)라고 합니다. 이 노드에는 최종 의사결정에 대한 정보가 담겨있습니다. 마지막으로 뿌리 노드와 말단노드를 제외한 모든 노드를 중간 노드(internal node)라 부르고 뿌리 노드부터 말단 노드까지의 노드 수를 깊이(depth)라고 합니다.
의사결정나무는 말단 노드에 존재하는 의사결정 결과를 보다 순수(pure)하게 만드는 모델입니다. 다만 그 결과를 만들기까지 어떤 변수를 이용해서 나눌 것이며, 어떤 기준을 두는가가 모델의 중요한 이슈가 되는겁니다. 이러한 기준으로는 어떻게 노드를 나눌 것인지, 여러 형태의 데이터를 다룰 수 있는지, 과적합을 어떻게 방지할 지 등등 여러개가 있고 주로 사용하는 알고리즘은 아래와 같습니다.
출처: Jaeyoon Han님의 2017 machine learning study 자료


﻿​이부분 차이에 대한 설명이 모호한거 같아서 추가 할 예정

● ID3 (Iterative Dichotomizer 3)
● CART (Classification and Regression Tree)
● C4.5
● C5
● CHAID

"참고로 이 중에서 C4.5, C5.0 알고리즘을 가장 많이 사용한다고 한다.
ID3=> C4.5 => C5.0 순으로 보완하면서 개발하여서 순서대로 학습을 해야 한다고 한다.



그리고 ID3,C4.5, C5.0 은 머신러닝 분야 개발이고,

CART,CHAID 알고리즘은 통계학 분야의 알고리즘이라고 한다"


![](https://choco9966.github.io/Team-EDA/6week/image/4.png) 

출처: http://blog.naver.com/trashx/60099037740 각 알고리즘 별 내용도 나와있음.

[출처] 기계학습 알고리즘 - 의사 결정 나무 (Tree Models /Decision tree),https://blog.naver.com/lk3436/220233499988|작성자 RDA"






![](https://choco9966.github.io/Team-EDA/6week/image/5.png) 
참고1: https://stackoverflow.com/questions/9979461/different-decision-tree-algorithms-with-comparison-of-complexity-or-performance

참고2: https://www.quora.com/Do-different-decision-tree-algorithms-offer-significant-differences-in-performance



출처: Jaeyoon Han님의 2017 machine learning study 자료


추후 링크(올릴예정)인 machine learning in action ch3에서 ID3에 대해, ch9에서는 CART를 파이썬으로 보다 상세하게 다룰 에정입니다.

CART (Classification and Regression Tree)​

이진분류를 사용하여 discrete target variable(Classification Tree)와 continuous target variable(Regression Tree)를 예측하는것을 의미합니다. (처음 공부할때는 이거를 잘 몰라서 헷갈렸습니다 ㅋㅋㅋ)

1.Measuring Impurity

①Gini Index for rectangle A containing m records
![](https://choco9966.github.io/Team-EDA/6week/image/1.jpg) 




p = proportion of cases in rectangle A that belong to class k

②Entropy Index for rectangle A containing m records
![](https://choco9966.github.io/Team-EDA/6week/image/2.jpg) 



p = proportion of cases in rectangle A that belong to class k




impurity measures

![](https://choco9966.github.io/Team-EDA/6week/image/6.png) 

출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea //참고로 이거 잘못된거 같은데 지금 새벽1시라 기억이 잘 안남. 추후 생각나면 수정함


한글로 번역해 줘야 하는데... 다들 영어 되시지요??? 믿습니다!!!
2.Impurity and Recursive Partitioning
●Apply a ‘Greedy Search’ strategy
●In each step, we find the split which reduces the impurity measure most
![](https://choco9966.github.io/Team-EDA/6week/image/7.png) 

●For binary categorical variables, it is easy to set ‘Left’ and ‘Right’
●What to do when a numerical input variable exist?
●Possible split values (cut-off) value can be given by sorting
출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 

3.Decision Tree Learning

●Building a Decision Tree

1.First test all attributes and select the on that would function as the best root;

2.Break-up the training set into subsets based on the branches of the root node;

3.Test the remaining attributes to see which ones fit best underneath the branches 
of the root node;

4.Continue this process for all other branches until

a.all examples of a subset are of one type

b.there are no examples left (return majority classification of the parent)

c.there are no more attributes left (default value should be majority classification)


●Determining which attribute is best (Entropy & Gain)
●Entropy (E) is the minimum number of bits needed in order to classify an arbitrary example as yes or no
●E(S) = Sci=1 –pi log2 pi ,
●Where S is a set of training examples,
●c is the number of classes, and
●pi is the proportion of the training set that is of class i
●For our entropy equation 0 log2 0 = 0
●The information gain G(S,A) where A is an attribute
●G(S,A) º E(S) - Sv in Values(A)     (|Sv|  /  |S|)  * E(Sv) 
출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 

4.Example

E(S) = E([9+,5-]) = (-9/14 log2 9/14) + (-5/14 log2 5/14) = 0.94
●Next we will need to calculate the information gain G(S,A) for each attribute A where A is taken from the set 
{Outlook, Temperature, Humidity, Wind}.
1) Outlook
●G(S,Outlook) = E(S) – [5/14 * E(Outlook=sunny) + 4/14 * E(Outlook = overcast) + 5/14 * E(Outlook=rain)]
●G(S,Outlook) = E([9+,5-]) – [5/14*E(2+,3-) + 4/14*E([4+,0-]) + 5/14*E([3+,2-])]
●G(S,Outlook) = 0.94 – [5/14*0.971 + 4/14*0.0 + 5/14*0.971]
●G(S,Outlook) = 0.246  <------ 제일 큼!!!
2) Temperature
●G(S,Temperature) = 0.94 – [4/14*E(Temperature=hot) +   6/14*E(Temperature=mild) +   4/14*E(Temperature=cool)]
●G(S,Temperature) = 0.94 – [4/14*E([2+,2-]) +     6/14*E([4+,2-]) + 4/14*E([3+,1-])]
●G(S,Temperature) = 0.94 – [4/14 + 6/14*0.918 + 4/14*0.811] 
●G(S,Temperature) = 0.029

3) Humidity
●G(S,Humidity) = 0.94 – [7/14*E(Humidity=high) + 7/14*E(Humidity=normal)]
●G(S,Humidity = 0.94 – [7/14*E([3+,4-]) + 7/14*E([6+,1-])]
●G(S,Humidity = 0.94 – [7/14*0.985 + 7/14*0.592]
●G(S,Humidity) = 0.1515
4) Wind
●G(S,Wind) = 0.94 – [8/14*0.811 + 6/14*1.00]
●G(S,Wind) = 0.048

![](https://choco9966.github.io/Team-EDA/6week/image/8.png) 

출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 



의사결정 나무의 장점과 단점 

1. 장점

사용하고,이해하기 쉽다. 규칙들을 생성하고 해석하기 매우 쉽다. 변수 선택과 축소가 자동적이다.

통계적인 가정이 필요하지 않다.  결측치를 다루지 않아도 된다.

2. 단점

수평 또는 수직 분할로 잘 포착되지 않은 구조의 데이터가있는 곳에서는 성능이 좋지 않을 수 있습니다.

프로세스가 한 번에 하나의 변수를 처리하기 때문에 변수 간의 상호 작용을 캡처 할 수 없습니다.

출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 



Random Forests ﻿

오버피팅 문제를 극복하기 위해 Bootstrapping(bagging)을 통해 만든 의사결정나무.







![](https://choco9966.github.io/Team-EDA/6week/image/9.png) 
출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 


![](https://choco9966.github.io/Team-EDA/6week/image/10.png) 
출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea 

Random Forest의 장점과 단점 

1. 장점

의사결정나무보다 높은 예측력. 일반화가 되어 과적합 가능성이 낮음

2. 단점

블랙박스 모델(모델의 설명력이 떨어짐)







참고: Classification과 Regression Tree 차이점.




![](https://choco9966.github.io/Team-EDA/6week/image/11.png) 

출처: https://webcache.googleusercontent.com/search?q=cache:JMex0zf_qCQJ:https://www.casact.org/education/specsem/f2005/handouts/cart.ppt+&cd=22&hl=ko&ct=clnk&gl=kr 



뒤에는 boosting 기법도 나오는데, boosting 의미와 그 중 하나
인 adaboost는 https://blog.naver.com/choco_9966/221248041919에서 다루었습니다. 하...xgboost랑 gradient boost까지 해야 할지는 고민입니다.

● 부록

출처: https://stats.stackexchange.com/questions/22035/partitioning-trees-in-r-party-vs-rpart

Partitioning trees in R: party vs. rpart
[NB: See update 1 below.] I find that the methodology for rpart is far easier to explain than party. The latter, however, is much more sophisticated and likely to give better models. The way I sometimes explain party is to speak of it as basis for producing local linear (or GLM) models. I build up to this by pointing out that the results for rpart are constant across all elements that fall into the leaf node, i.e. the box/region bounded by the splits. Even if there might be improvements via local models, you don't get anything but a constant prediction.

In contrast, party develops the splits to potentially optimize the models for the regions. It is actually using a different criteria than model optimality, but you need to gauge your own capacity for explaining the difference to determine whether you can explain it well. The papers for it are pretty accessible for a researcher, but may be quite challenging for someone not willing to consider simpler methods like random forests, boosting, etc. Mathematically, I think that party is more sophisticated... Nonetheless, CART models are easier to explain, both in terms of methodology and results, and these provide a decent stepping stone for introducing more sophisticated tree-based models.

In short, I would say that you have to do rpart for clarity, and you can use party for accuracy / peformance, but I wouldn't introduce party without introducing rpart.

Update 1. I based my answer on my understanding of party as it was a year or two ago. It has grown up quite a bit, but I would modify my answer to say that I'd still recommend rpart for its brevity and legacy, should "non-fancy" be an important criterion for your client/collaborator. Yet, I would try to migrate to using more functionality from party, after having introduced someone to rpart. It's better to start small, with loss functions, splitting criteria, etc., in a simple context, before introducing a package and methodology that involve far more involved concepts.