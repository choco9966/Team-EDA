# 6주일차: Decision Tree(의사결정나무)
### Author: 김현우,박주연,이주영,이지예,주진영,홍정아

---
# 의사결정나무(Decision Tree)

#### 의사결정나무는 마치 스무고개를 하듯이 **Rules**에 의해서 **Predictors**로 **Target**을 예측하는 모델입니다.

![](https://choco9966.github.io/Team-EDA/6week/image/1.png)
*출처: Jaeyoon Han님의 2017 machine learning study 자료*

위의 예에서는 **날씨, 바람, 온도, 습도** 4가지에 따라 Golf를 할지, 안 할지를 예측하는것 입니다.

- 뿌리 노드(root node): 의사결정나무에서 맨위에 있는 사각형. 위의 예시에서는 **Yes=9, No=5**를 의미.

- 가지(branch): 노드와 노드를 잇는 선. 위의 예시에서는 **화살표**를 의미

- 잎사귀 노드(leaf node) 혹은 말단 노드(terminal node): 가지가 밑으로 더 이상 생성되지 않는 노드. 위의 예시에서는 마지막 줄 **FALSE,TRUE,High,Normal**을 의미.

- 중간 노드(internal node): 뿌리 노드와 말단 노드를 제외한 모든 노드. 위의 예시에서는 **Sunny,Overcast,Rainy**

- 깊이(depth): 뿌리 노드부터 말단 노드까지의 노드 수. 위의 예시에서는 **깊이 = 3**

의사결정나무는 말단 노드에 존재하는 의사결정 결과를 보다 **순수(pure)** 하게 만드는 모델입니다. 다만 그 결과를 만들기까지 어떤 변수를 이용해서 나눌 것이며, 어떤 기준을 두는가가 모델의 중요한 이슈가 되는겁니다. 이러한 기준으로는 어떻게 노드를 나눌 것인지, 여러 형태의 데이터를 다룰 수 있는지, 과적합을 어떻게 방지할 지 등등 여러개가 있고 주로 사용하는 알고리즘은 아래와 같습니다.

*출처: Jaeyoon Han님의 2017 machine learning study 자료*

#### 주로 사용하는 decision Tree algorithm의 비교

- ID3 (Iterative Dichotomizer 3)
- CART (Classification and Regression Tree)
- C4.5
- C5
- CHAID

참고로 이 중에서 C4.5, C5.0 알고리즘을 가장 많이 사용하고

ID3 => C4.5 => C5.0 순으로 보완하면서 개발하여서 순서대로 학습을 해야 한다고 한다.
![](https://choco9966.github.io/Team-EDA/6week/image/5.png) 

*출처: Jaeyoon Han님의 2017 machine learning study 자료*

*참고1: https://stackoverflow.com/questions/9979461/different-decision-tree-algorithms-with-comparison-of-complexity-or-performance*

*참고2: https://www.quora.com/Do-different-decision-tree-algorithms-offer-significant-differences-in-performance*

*참고3: http://blog.naver.com/trashx/60099037740 각 알고리즘 별 내용도 나와있음.*

*참고4: https://blog.naver.com/lk3436/220233499988|작성자 RDA"*

---

### 방법1. 분류 예측

##### 정보 이득 공식
![](https://choco9966.github.io/Team-EDA/6week/image/7.png) 

정보 이득(Information gain) = 불순도(impurity) - p * 불순도(s1) - (1-p) *  불순도(s2)

- s1: 기준 값과 동일 값을 갖는 부분 집합
- s2: 기준 값과 다른 값을 갖는 부분 집합
- p: s1의 비율

##### 불순도 종류 및 공식
- 지니 불순도 (Gini Index for rectangle A containing m records)
![](https://choco9966.github.io/Team-EDA/6week/image/1.jpg) 

p = proportion of cases in rectangle A that belong to class k

- 엔트로피 (Entropy Index for rectangle A containing m records)
![](https://choco9966.github.io/Team-EDA/6week/image/2.jpg) 

p = proportion of cases in rectangle A that belong to class k

- 분산

---

### 분류 예측 예시)
![](https://choco9966.github.io/Team-EDA/6week/image/1.png)


E(S) = E([9+,5-]) = (-9/14 log2 9/14) + (-5/14 log2 5/14) = 0.94 

- ##### Outlook: 
G(S,Outlook) = E(S) – [5/14 x E(Outlook=sunny) + 4/14 x E(Outlook = overcast) + 5/14 x E(Outlook=rain)] 
G(S,Outlook) = E([9+,5-]) – [5/14 x E(2+,3-) + 4/14 x E([4+,0-]) + 5/14 x E([3+,2-])] 
G(S,Outlook) = 0.94 – [5/14 x 0.971 + 4/14 x 0.0 + 5/14 x 0.971] 
G(S,Outlook) = 0.246 **<------ 제일 큼!!!** 그래서 첫번째 분류로 선택.

- ##### Temperature: 
G(S,Temperature) = 0.94 – [4/14 x E(Temperature=hot) + 6/14 x E(Temperature=mild) + 4/14 x E(Temperature=cool)] 
G(S,Temperature) = 0.94 – [4/14 x E([2+,2-]) + 6/14 x E([4+,2-]) + 4/14 x E([3+,1-])] 
G(S,Temperature) = 0.94 – [4/14 + 6/14 x 0.918 + 4/14 x 0.811] 
G(S,Temperature) = 0.029

- ##### Humidity: 
G(S,Humidity) = 0.94 – [7/14 x E(Humidity=high) + 7/14 x E(Humidity=normal)] 
G(S,Humidity = 0.94 – [7/14 x E([3+,4-]) + 7/14 x E([6+,1-])] 
G(S,Humidity = 0.94 – [7/14 x 0.985 + 7/14 x 0.592] 
G(S,Humidity) = 0.1515

- ##### Wind: 
G(S,Wind) = 0.94 – [8/14 x 0.811 + 6/14 x 1.00] 
G(S,Wind) = 0.048

![](https://choco9966.github.io/Team-EDA/6week/image/12.PNG)

E(Outlook=sunny) = 0.971 , E(Outlook = overcast) = 0 , E(Outlook=rain) = 0.971 로 overcast는 더 이상 분류할 수 없고, 남은 sunny, rain의 경우를 마찬가지로 분류하면 처음 예시 그래프처럼 나온다.

#### 의사결정 나무의 장점과 단점

- 장점
사용하고,이해하기 쉽다. 규칙들을 생성하고 해석하기 매우 쉽다. 변수 선택과 축소가 자동적이다.

통계적인 가정이 필요하지 않다. 결측치를 다루지 않아도 된다.

- 단점
수평 또는 수직 분할로 잘 포착되지 않은 구조의 데이터가있는 곳에서는 성능이 좋지 않을 수 있습니다.

프로세스가 한 번에 하나의 변수를 처리하기 때문에 변수 간의 상호 작용을 캡처 할 수 없습니다.


---

### 수치 예측 예시) CART(Classification And Regression Tree)

|속성1|속성2|결과값|
|---|---|---|
|1|1|1.5|
|1|2|2.5|
|2|3|7|
|2|4|9|

#### CART 진행순서
- ##### 데이터 집합을 가장 잘 분할하는 속성의 값을 찾음. 결과 값이 모두 같으면, 분할하는 (속성,기준값)이 없음
- ##### (속성,기준값)이 없으면 데이터 집합의 결과 평균을 값으로 갖는 노드를 생성해서 리턴
- ##### (속성,기준값)이 존재하면, (속성,기준값)을 기준으로 이진 분할
  - ##### 노드 생성: (속성,기준값)을 가짐
    - ##### 속성의 값이 기준값 보다 크면 왼쪽 자식 노드 생성(2번재귀실행), 같거나 작으면 오른쪽 자식 노드 생성(2번재귀실행)
  - ##### 노드 리턴
 
 
|속성1|속성2|결과값|
|---|---|---|
|1|1|1.5|
|1|2|2.5|
|2|3|7|
|2|4|9|

여기서 **속성1**은 기준값을 **1** 과 **2** 를 가지고, **속성2**는 기준값을 **1, 2, 3, 4** 를 갖는다.

- (속성1, 기준값=1)로 나눌떄 평균제곱오류 = 결과값 1.5와 2.5의 평균제곱오류 **<기준값 왼쪽>** + 결과값 7과 9의 평균제곱오류 **<기준값 오른쪽>** = ((2-1.5)^2 +(2-2.5)^2) + ((8-7)^2 + (8-9)^2) = 0.5 + 2 = 2.5


- (속성2, 기준값=1)로 나눌떄 평균제곱오류 = 결과값 1.5의 평균제곱오류 **<기준값 왼쪽>** + 결과값 2.5,7과 9의 평균제곱오류 **<기준값 오른쪽>** = 22.166667

- (속성2, 기준값=2)로 나눌떄 평균제곱오류 = 결과값 1.5와 2.5의 평균제곱오류 **<기준값 왼쪽>** + 결과값 7과 9의 평균제곱오류 **<기준값 오른쪽>**  = 2.5

- (속성2, 기준값=3)로 나눌떄 평균제곱오류 = 결과값 1.5, 2.5, 7의 평균제곱오류 **<기준값 왼쪽>** + 결과값 9의 평균제곱오류 **<기준값 오른쪽>** = 17.16667

![](https://choco9966.github.io/Team-EDA/6week/image/13.PNG)

*출처: http://javacan.tistory.com/entry/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%B8-%EC%95%A1%EC%85%98-%EC%A0%95%EB%A6%AC-9%EC%9E%A5-%ED%8A%B8%EB%A6%AC-%EA%B8%B0%EB%B0%98-%ED%9A%8C%EA%B7%80*

마찬가지로 계속 진행하면 아래와 같은 결과를 얻을 수 있다.

![](https://choco9966.github.io/Team-EDA/6week/image/14.PNG)

*출처: http://javacan.tistory.com/entry/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%B8-%EC%95%A1%EC%85%98-%EC%A0%95%EB%A6%AC-9%EC%9E%A5-%ED%8A%B8%EB%A6%AC-%EA%B8%B0%EB%B0%98-%ED%9A%8C%EA%B7%80*

---
### Random Forests : 오버피팅 문제를 극복하기 위해 Bootstrapping(bagging)을 통해 만든 의사결정나무.

![](https://choco9966.github.io/Team-EDA/6week/image/9.png) 

*출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea*

decision tree를 여러개 만들어서 합친게 Random Forest라고 생각하면 되고, 아래는 Random Forest의 알고리즘이다.

![](https://choco9966.github.io/Team-EDA/6week/image/10.png) 

*출처: Kichun Lee, Ph.D. Hanyang University, Seoul, Korea*

#### Random Forest의 장점과 단점 

- 장점
의사결정나무보다 높은 예측력. 일반화가 되어 과적합 가능성이 낮음

- 단점
블랙박스 모델(모델의 설명력이 떨어짐)

---

앙생블(ensemble)기법을 보통 크게 1. bagging 2. boosting 으로 나눈다.
bagging은 위에 random forest에 보인것처럼 단순 평균이었다면, boosting은 가중치를 바꿔가면서 error를 줄이는 방식이다.
boosting 중 하나인 adaboost는 https://blog.naver.com/choco_9966/221248041919 에서 다루고, xgboost는 10주차에서 다룰 예정이다.

---
부록
#### R에서 패키지 차이.

출처: https://stats.stackexchange.com/questions/22035/partitioning-trees-in-r-party-vs-rpart

Partitioning trees in R: party vs. rpart
[NB: See update 1 below.] I find that the methodology for rpart is far easier to explain than party. The latter, however, is much more sophisticated and likely to give better models. The way I sometimes explain party is to speak of it as basis for producing local linear (or GLM) models. I build up to this by pointing out that the results for rpart are constant across all elements that fall into the leaf node, i.e. the box/region bounded by the splits. Even if there might be improvements via local models, you don't get anything but a constant prediction.

In contrast, party develops the splits to potentially optimize the models for the regions. It is actually using a different criteria than model optimality, but you need to gauge your own capacity for explaining the difference to determine whether you can explain it well. The papers for it are pretty accessible for a researcher, but may be quite challenging for someone not willing to consider simpler methods like random forests, boosting, etc. Mathematically, I think that party is more sophisticated... Nonetheless, CART models are easier to explain, both in terms of methodology and results, and these provide a decent stepping stone for introducing more sophisticated tree-based models.

In short, I would say that you have to do rpart for clarity, and you can use party for accuracy / peformance, but I wouldn't introduce party without introducing rpart.

Update 1. I based my answer on my understanding of party as it was a year or two ago. It has grown up quite a bit, but I would modify my answer to say that I'd still recommend rpart for its brevity and legacy, should "non-fancy" be an important criterion for your client/collaborator. Yet, I would try to migrate to using more functionality from party, after having introduced someone to rpart. It's better to start small, with loss functions, splitting criteria, etc., in a simple context, before introducing a package and methodology that involve far more involved concepts.
